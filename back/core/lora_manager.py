#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LoRAç®¡ç†å™¨
è´Ÿè´£ä»é…ç½®å®¢æˆ·ç«¯è·å–LoRAé…ç½®ï¼Œæ”¯æŒLoRAåˆ†ç»„å’Œæ’åº
"""

import os
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging

from config.settings import COMFYUI_LORAS_DIR

logger = logging.getLogger(__name__)


class LoraManager:
    """LoRAç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–LoRAç®¡ç†å™¨"""
        self._config_client = None
        self._local_loras_cache = {}
        self._last_local_scan = None
        # æ¨¡å‹æ˜ å°„å·²ç§»é™¤ï¼Œç°åœ¨å®Œå…¨ä¾èµ–é…ç½®å®¢æˆ·ç«¯åŠ¨æ€è·å–
        # æ‰€æœ‰æ¨¡å‹é…ç½®éƒ½é€šè¿‡é…ç½®å®¢æˆ·ç«¯ä»adminåç«¯è·å–
    
    def _get_config_client(self):
        """è·å–é…ç½®å®¢æˆ·ç«¯"""
        if self._config_client is None:
            try:
                from core.config_client import get_config_client
                self._config_client = get_config_client()
            except ImportError:
                # å¦‚æœé…ç½®å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè¿”å›None
                return None
        return self._config_client
    
    async def get_loras_from_config(self, base_model: Optional[str] = None) -> Dict[str, Any]:
        """ä»é…ç½®å®¢æˆ·ç«¯è·å–LoRAé…ç½®"""
        try:
            config_client = self._get_config_client()
            if config_client:
                config = await config_client.get_loras_config()
                
                # è·å–ç”Ÿå›¾é…ç½®ä¸­çš„LoRAæ’åº
                try:
                    image_gen_config = await config_client.get_image_gen_config()
                    lora_order = image_gen_config.get("lora_order", {})
                    logger.info(f"ğŸ” è·å–åˆ°LoRAæ’åºé…ç½®: {lora_order}")
                    print(f"ğŸ” è·å–åˆ°LoRAæ’åºé…ç½®: {lora_order}")
                except Exception as e:
                    logger.warning(f"è·å–LoRAæ’åºé…ç½®å¤±è´¥: {e}")
                    print(f"âŒ è·å–LoRAæ’åºé…ç½®å¤±è´¥: {e}")
                    lora_order = {}
                
                # å¦‚æœæŒ‡å®šäº†åŸºç¡€æ¨¡å‹ï¼Œè¿›è¡Œè¿‡æ»¤
                if base_model:
                    # ä½¿ç”¨codeå­—æ®µè¿›è¡Œæ¨¡å‹åŒ¹é…ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨nameå­—æ®µ
                    # æ›´å®½æ¾çš„è¿‡æ»¤é€»è¾‘ï¼šå¦‚æœbase_modelæ˜¯"æœªçŸ¥"æˆ–ç©ºï¼Œä¹ŸåŒ…å«è¿›æ¥
                    filtered_loras = [
                        lora for lora in config.get("loras", [])
                        if (lora.get("base_model") == base_model or 
                            lora.get("base_model") in ["æœªçŸ¥", "", None] or
                            base_model in lora.get("base_model", ""))
                    ]
                    
                    # åº”ç”¨æ’åºé…ç½®
                    if lora_order and base_model in lora_order:
                        model_lora_order = lora_order[base_model]
                        logger.info(f"ğŸ”§ åº”ç”¨LoRAæ’åºé…ç½®ï¼Œæ¨¡å‹: {base_model}, æ’åºåˆ—è¡¨: {model_lora_order}")
                        print(f"ğŸ”§ åº”ç”¨LoRAæ’åºé…ç½®ï¼Œæ¨¡å‹: {base_model}, æ’åºåˆ—è¡¨: {model_lora_order}")
                        print(f"ğŸ” æ’åºåˆ—è¡¨é•¿åº¦: {len(model_lora_order)}")
                        for i, item in enumerate(model_lora_order):
                            print(f"  {i}: '{item}'")
                        # æŒ‰é…ç½®çš„æ’åºé‡æ–°æ’åˆ—
                        def sort_key(lora):
                            # å°è¯•åŒ¹é…codeå­—æ®µå’Œnameå­—æ®µ
                            lora_code = lora.get("code")
                            lora_name = lora.get("name", "")
                            
                            # é¦–å…ˆå°è¯•åŒ¹é…codeå­—æ®µ
                            if lora_code and lora_code in model_lora_order:
                                order = model_lora_order.index(lora_code)
                                print(f"âœ… åŒ¹é…codeå­—æ®µ: {lora_code} -> æ’åº: {order}")
                                return order
                            
                           
                            
                            print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…: code={lora_code}, name={lora_name}")
                            return 999  # æœªé…ç½®çš„æ’åœ¨æœ€å
                        filtered_loras.sort(key=sort_key)
                    
                    config["loras"] = filtered_loras
                    config["filtered_by_model"] = base_model
                    config["actual_base_model"] = base_model
                    config["sort_applied"] = bool(lora_order and base_model in lora_order)
                
                return config
            else:
                # é…ç½®å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°æ‰«æ
                return await self._get_loras_from_local_scan(base_model)
        except Exception as e:
            logger.error(f"ä»é…ç½®è·å–LoRAå¤±è´¥: {e}")
            # é™çº§åˆ°æœ¬åœ°æ‰«æ
            return await self._get_loras_from_local_scan(base_model)
    
    async def _get_loras_from_local_scan(self, base_model: Optional[str] = None) -> Dict[str, Any]:
        """ä»æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿæ‰«æLoRA"""
        try:
            lora_dir = COMFYUI_LORAS_DIR
            
            if not lora_dir.exists():
                logger.warning(f"LoRAç›®å½•ä¸å­˜åœ¨: {lora_dir}")
                return {
                    "loras": [],
                    "grouped_by_model": {},
                    "config_source": "local_scan",
                    "error": "LoRAç›®å½•ä¸å­˜åœ¨"
                }
            
            loras = []
            grouped_by_model = {}
            
            # æ‰«æLoRAæ–‡ä»¶
            for file_path in lora_dir.iterdir():
                if file_path.is_file() and file_path.suffix.lower() in ['.safetensors', '.ckpt', '.pt']:
                    # å°è¯•ä»æ–‡ä»¶åæ¨æ–­åŸºç¡€æ¨¡å‹
                    inferred_model = self._infer_base_model_from_filename(file_path.name)
                    
                    lora_data = {
                        "name": file_path.name,
                        "display_name": self._generate_display_name(file_path.name),
                        "base_model": inferred_model,
                        "available": True,
                        "file_size": file_path.stat().st_size,
                        "file_path": str(file_path),
                        "file_type": file_path.suffix.lower()
                    }
                    
                    # å¦‚æœæŒ‡å®šäº†åŸºç¡€æ¨¡å‹ï¼Œè¿›è¡Œè¿‡æ»¤
                    if base_model and inferred_model != base_model:
                        continue
                    
                    loras.append(lora_data)
                    
                    # æŒ‰æ¨¡å‹åˆ†ç»„
                    if inferred_model not in grouped_by_model:
                        grouped_by_model[inferred_model] = []
                    grouped_by_model[inferred_model].append(file_path.name)
            
            # æŒ‰åç§°æ’åº
            loras.sort(key=lambda x: x["name"])
            
            return {
                "loras": loras,
                "grouped_by_model": grouped_by_model,
                "config_source": "local_scan",
                "filtered_by_model": base_model,
                "total_count": len(loras)
            }
        except Exception as e:
            logger.error(f"æœ¬åœ°LoRAæ‰«æå¤±è´¥: {e}")
            return {
                "loras": [],
                "grouped_by_model": {},
                "config_source": "error",
                "error": str(e)
            }
    
    def _infer_base_model_from_filename(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæ¨æ–­åŸºç¡€æ¨¡å‹"""
        filename_lower = filename.lower()
        
        # æ ¹æ®æ–‡ä»¶åä¸­çš„å…³é”®è¯æ¨æ–­æ¨¡å‹ç±»å‹
        if "flux" in filename_lower:
            return "flux-dev"
        elif "qwen" in filename_lower:
            return "qwen-image"
        elif "wan" in filename_lower:
            return "wan2.2-video"
        elif "gemini" in filename_lower:
            return "gemini-image"
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ¨¡å‹ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹
            available_models = list(self.model_mapping.keys())
            if available_models:
                return available_models[0]
            return None
    
    def _generate_display_name(self, filename: str) -> str:
        """ç”Ÿæˆæ˜¾ç¤ºåç§°"""
        # ç§»é™¤æ–‡ä»¶æ‰©å±•å
        name = Path(filename).stem
        
        # æ›¿æ¢ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦ä¸ºç©ºæ ¼
        name = name.replace("_", " ").replace("-", " ")
        
        # é¦–å­—æ¯å¤§å†™
        name = name.title()
        
        return name
    
    async def get_loras_by_model(self, base_model: str) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šæ¨¡å‹çš„LoRAåˆ—è¡¨"""
        config = await self.get_loras_from_config(base_model)
        return config.get("loras", [])
    
    async def get_all_loras(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰LoRAé…ç½®"""
        return await self.get_loras_from_config()
    
    def get_local_loras(self) -> List[Dict[str, Any]]:
        """è·å–æœ¬åœ°LoRAæ–‡ä»¶åˆ—è¡¨ï¼ˆåŒæ­¥æ–¹æ³•ï¼Œé™çº§ä½¿ç”¨ï¼‰"""
        try:
            lora_dir = COMFYUI_LORAS_DIR
            
            if not lora_dir.exists():
                return []
            
            loras = []
            for file_path in lora_dir.iterdir():
                if file_path.is_file() and file_path.suffix.lower() in ['.safetensors', '.ckpt', '.pt']:
                    loras.append({
                        "name": file_path.name,
                        "path": str(file_path),
                        "size": file_path.stat().st_size,
                        "type": file_path.suffix.lower()
                    })
            
            return loras
        except Exception as e:
            logger.error(f"è·å–æœ¬åœ°LoRAå¤±è´¥: {e}")
            return []
    
    async def check_lora_availability(self, lora_name: str, base_model: Optional[str] = None) -> bool:
        """æ£€æŸ¥LoRAå¯ç”¨æ€§"""
        try:
            config = await self.get_loras_from_config(base_model)
            loras = config.get("loras", [])
            
            for lora in loras:
                if lora.get("name") == lora_name:
                    return lora.get("available", False)
            
            return False
        except Exception as e:
            logger.error(f"æ£€æŸ¥LoRAå¯ç”¨æ€§å¤±è´¥: {e}")
            return False
    
    async def get_lora_info(self, lora_name: str, base_model: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """è·å–LoRAè¯¦ç»†ä¿¡æ¯"""
        try:
            config = await self.get_loras_from_config(base_model)
            loras = config.get("loras", [])
            
            for lora in loras:
                if lora.get("name") == lora_name:
                    return lora
            
            return None
        except Exception as e:
            logger.error(f"è·å–LoRAä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def refresh_local_cache(self):
        """åˆ·æ–°æœ¬åœ°ç¼“å­˜"""
        self._local_loras_cache.clear()
        self._last_local_scan = None
        logger.info("LoRAæœ¬åœ°ç¼“å­˜å·²åˆ·æ–°")


# å…¨å±€LoRAç®¡ç†å™¨å®ä¾‹
_lora_manager: Optional[LoraManager] = None


def get_lora_manager() -> LoraManager:
    """è·å–LoRAç®¡ç†å™¨å®ä¾‹"""
    global _lora_manager
    if _lora_manager is None:
        _lora_manager = LoraManager()
    return _lora_manager


# ä¾¿æ·å‡½æ•°
async def get_loras_from_config(base_model: Optional[str] = None) -> Dict[str, Any]:
    """ä»é…ç½®å®¢æˆ·ç«¯è·å–LoRAé…ç½®"""
    manager = get_lora_manager()
    return await manager.get_loras_from_config(base_model)


async def get_loras_by_model(base_model: str) -> List[Dict[str, Any]]:
    """è·å–æŒ‡å®šæ¨¡å‹çš„LoRAåˆ—è¡¨"""
    manager = get_lora_manager()
    return await manager.get_loras_by_model(base_model)


async def get_all_loras() -> Dict[str, Any]:
    """è·å–æ‰€æœ‰LoRAé…ç½®"""
    manager = get_lora_manager()
    return await manager.get_all_loras()


def get_local_loras() -> List[Dict[str, Any]]:
    """è·å–æœ¬åœ°LoRAæ–‡ä»¶åˆ—è¡¨ï¼ˆåŒæ­¥æ–¹æ³•ï¼Œé™çº§ä½¿ç”¨ï¼‰"""
    manager = get_lora_manager()
    return manager.get_local_loras()


async def check_lora_availability(lora_name: str, base_model: Optional[str] = None) -> bool:
    """æ£€æŸ¥LoRAå¯ç”¨æ€§"""
    manager = get_lora_manager()
    return await manager.check_lora_availability(lora_name, base_model)


async def get_lora_info(lora_name: str, base_model: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """è·å–LoRAè¯¦ç»†ä¿¡æ¯"""
    manager = get_lora_manager()
    return await manager.get_lora_info(lora_name, base_model)


def refresh_lora_cache():
    """åˆ·æ–°LoRAç¼“å­˜"""
    manager = get_lora_manager()
    manager.refresh_local_cache()
