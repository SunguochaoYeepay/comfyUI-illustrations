#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å·¥ä½œæµæ¨¡æ¿ç®¡ç†å™¨
è´Ÿè´£åˆ›å»ºå’Œè‡ªå®šä¹‰Flux Kontextå·¥ä½œæµ
"""

import json
import random
from pathlib import Path
from typing import Any, Dict

from config.settings import (
    TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT, 
    DEFAULT_STEPS, DEFAULT_COUNT, COMFYUI_MAIN_OUTPUT_DIR
)


class WorkflowTemplate:
    """å·¥ä½œæµæ¨¡æ¿ç®¡ç†å™¨ï¼Œè´Ÿè´£åˆ›å»ºå’Œè‡ªå®šä¹‰å¤šç§æ¨¡å‹çš„å·¥ä½œæµ"""
    
    def __init__(self, template_path: str = None):
        """åˆå§‹åŒ–å·¥ä½œæµæ¨¡æ¿
        
        Args:
            template_path: æ¨¡æ¿æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤Fluxæ¨¡æ¿ï¼‰
        """
        self.template_path = template_path
        if template_path:
            try:
                with open(template_path, 'r', encoding='utf-8') as f:
                    self.template = json.load(f)
            except FileNotFoundError:
                raise FileNotFoundError(f"å·¥ä½œæµæ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
            except json.JSONDecodeError:
                raise ValueError(f"å·¥ä½œæµæ¨¡æ¿æ–‡ä»¶æ ¼å¼é”™è¯¯: {template_path}")
        else:
            # ä½¿ç”¨é»˜è®¤Fluxæ¨¡æ¿
            self.template = None
    
    def customize_workflow(self, reference_image_path: str, description: str, parameters: Dict[str, Any], model_name: str = "flux1-dev"):
        """è‡ªå®šä¹‰å·¥ä½œæµå‚æ•° - æ”¯æŒå¤šç§æ¨¡å‹
        
        Args:
            reference_image_path: å‚è€ƒå›¾åƒè·¯å¾„
            description: å›¾åƒæè¿°
            parameters: ç”Ÿæˆå‚æ•°
            model_name: æ¨¡å‹åç§°ï¼ˆé»˜è®¤flux1-devï¼‰
        """
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å·¥ä½œæµæ¨¡æ¿
        from core.model_manager import get_model_config, ModelType
        
        model_config = get_model_config(model_name)
        if not model_config or not model_config.available:
            print(f"âš ï¸ æ¨¡å‹ {model_name} ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤Fluxæ¨¡å‹")
            model_config = get_model_config("flux1-dev")
        
        print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: {model_config.display_name}")
        
        if model_config.model_type == ModelType.FLUX:
            return self._create_flux_workflow(reference_image_path, description, parameters, model_config)
        elif model_config.model_type == ModelType.QWEN:
            return self._create_qwen_workflow(reference_image_path, description, parameters, model_config)
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_config.model_type}")
            return self._create_flux_workflow(reference_image_path, description, parameters, model_config)
    
    def _create_flux_workflow(self, reference_image_path: str, description: str, parameters: Dict[str, Any], model_config):
        """åˆ›å»ºFluxå·¥ä½œæµ"""
        # åˆ›å»ºä¸€ä¸ªä¼˜åŒ–çš„Flux Kontextå·¥ä½œæµï¼Œå‚è€ƒQwenå·¥ä½œæµçš„è®¾è®¡
        
        workflow = {
            "6": {
                "inputs": {
                    "text": description,
                    "clip": ["38", 0]  # é»˜è®¤è¿æ¥åˆ°DualCLIPLoader
                },
                "class_type": "CLIPTextEncode",
                "_meta": {"title": "CLIPæ–‡æœ¬ç¼–ç å™¨"}
            },
            "8": {
                "inputs": {
                    "samples": ["31", 0],
                    "vae": ["39", 0]
                },
                "class_type": "VAEDecode",
                "_meta": {"title": "VAEè§£ç "}
            },
            "31": {
                "inputs": {
                    "seed": parameters.get("seed", random.randint(1, 2**32 - 1)),
                    "steps": parameters.get("steps", DEFAULT_STEPS),
                    "cfg": 1,
                    "sampler_name": "euler",
                    "scheduler": "simple",
                    "denoise": 1,
                    "batch_size": parameters.get("count", DEFAULT_COUNT),
                    "model": ["37", 0],  # é»˜è®¤è¿æ¥åˆ°UNETLoader
                    "positive": ["35", 0],
                    "negative": ["135", 0],
                    "latent_image": ["124", 0]
                },
                "class_type": "KSampler",
                "_meta": {"title": "Ké‡‡æ ·å™¨"}
            },
            "35": {
                "inputs": {
                    "guidance": 2.5,
                    "conditioning": ["177", 0]
                },
                "class_type": "FluxGuidance",
                "_meta": {"title": "Fluxå¼•å¯¼"}
            },
            "37": {
                "inputs": {
                    "unet_name": model_config.unet_file,
                    "weight_dtype": "default"
                },
                "class_type": "UNETLoader",
                "_meta": {"title": "UNETåŠ è½½å™¨"}
            },
            "38": {
                "inputs": {
                    "clip_name1": "clip_l.safetensors",
                    "clip_name2": "t5xxl_fp8_e4m3fn_scaled.safetensors",
                    "type": "flux",
                    "device": "default"
                },
                "class_type": "DualCLIPLoader",
                "_meta": {"title": "åŒCLIPåŠ è½½å™¨"}
            },
            "39": {
                "inputs": {
                    "vae_name": "ae.safetensors"
                },
                "class_type": "VAELoader",
                "_meta": {"title": "VAEåŠ è½½å™¨"}
            },
            "42": {
                "inputs": {
                    "width": TARGET_IMAGE_WIDTH,
                    "height": TARGET_IMAGE_HEIGHT,
                    "batch_size": 1,
                    "color": 0
                },
                "class_type": "EmptyImage",
                "_meta": {"title": "ç©ºå›¾åƒ"}
            },
            "124": {
                "inputs": {
                    "pixels": ["42", 0],
                    "vae": ["39", 0]
                },
                "class_type": "VAEEncode",
                "_meta": {"title": "VAEç¼–ç "}
            },
            "135": {
                "inputs": {
                    "conditioning": ["6", 0]
                },
                "class_type": "ConditioningZeroOut",
                "_meta": {"title": "æ¡ä»¶é›¶åŒ–"}
            },
            "136": {
                "inputs": {
                    "filename_prefix": "yeepay/yeepay",
                    "images": ["8", 0],
                    "save_all": True
                },
                "class_type": "SaveImage",
                "_meta": {"title": "ä¿å­˜å›¾åƒ"}
            },
            "177": {
                "inputs": {
                    "conditioning": ["6", 0],
                    "latent": ["124", 0]
                },
                "class_type": "ReferenceLatent",
                "_meta": {"title": "ReferenceLatent"}
            }
        }
        
        print(f"âœ… åˆ›å»ºä¼˜åŒ–å·¥ä½œæµï¼ŒåŒ…å« {len(workflow)} ä¸ªèŠ‚ç‚¹")
        print(f"ğŸ“‹ å·¥ä½œæµèŠ‚ç‚¹: {list(workflow.keys())}")
        
        # å¤„ç†LoRAé…ç½® - å‚è€ƒQwenå·¥ä½œæµçš„ä¼˜åŒ–è®¾è®¡
        loras = parameters.get("loras", [])
        if loras and len(loras) > 0:
            print(f"ğŸ¨ æ£€æµ‹åˆ° {len(loras)} ä¸ªLoRAé…ç½®")
            
            # é™åˆ¶æœ€å¤š4ä¸ªLoRA
            loras = loras[:4]
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨Lora Loader Stack (rgthree)
            use_lora_stack = False
            try:
                # å°è¯•ä½¿ç”¨Lora Loader Stackï¼Œå¦‚æœå¯ç”¨çš„è¯
                # è¿™é‡Œæˆ‘ä»¬å…ˆä½¿ç”¨ä¼ ç»Ÿçš„LoraLoaderæ–¹å¼ï¼Œä½†ç»“æ„æ›´ä¼˜åŒ–
                use_lora_stack = False  # æš‚æ—¶ä¸ä½¿ç”¨ï¼Œç¡®ä¿å…¼å®¹æ€§
            except:
                use_lora_stack = False
            
            if use_lora_stack:
                # ä½¿ç”¨Lora Loader Stack (rgthree) - å‚è€ƒQwenå·¥ä½œæµ
                print("ğŸ¨ ä½¿ç”¨Lora Loader Stack (rgthree) èŠ‚ç‚¹")
                # TODO: å®ç°Lora Loader Stacké€»è¾‘
            else:
                # ä½¿ç”¨ä¼˜åŒ–çš„ä¼ ç»ŸLoraLoaderæ–¹å¼
                print("ğŸ¨ ä½¿ç”¨ä¼˜åŒ–çš„ä¼ ç»ŸLoraLoaderæ–¹å¼")
                
                current_model_node = "37"  # UNETLoader
                current_clip_node = "38"   # DualCLIPLoader
                
                for i, lora_config in enumerate(loras):
                    if not lora_config.get("enabled", True):
                        print(f"â­ï¸ è·³è¿‡ç¦ç”¨çš„LoRA {i+1}: {lora_config.get('name', 'unknown')}")
                        continue
                    
                    lora_node_id = str(50 + i)  # 50, 51, 52, 53
                    lora_name = lora_config.get("name", "")
                    strength_model = lora_config.get("strength_model", 1.0)
                    strength_clip = lora_config.get("strength_clip", 1.0)
                    trigger_word = lora_config.get("trigger_word", "")
                    
                    print(f"ğŸ¨ æ·»åŠ LoRA {i+1}: {lora_name} (UNET: {strength_model}, CLIP: {strength_clip})")
                    
                    # æ·»åŠ LoRAèŠ‚ç‚¹ - ä¼˜åŒ–è¿æ¥æ–¹å¼
                    workflow[lora_node_id] = {
                        "inputs": {
                            "model": [current_model_node, 0],
                            "clip": [current_clip_node, 0],  # DualCLIPLoaderçš„CLIPè¾“å‡ºæ˜¯ç«¯å£0
                            "lora_name": lora_name,
                            "strength_model": strength_model,
                            "strength_clip": strength_clip
                        },
                        "class_type": "LoraLoader",
                        "_meta": {"title": f"LoRAåŠ è½½å™¨{i+1}"}
                    }
                    
                    # æ›´æ–°å½“å‰èŠ‚ç‚¹å¼•ç”¨
                    current_model_node = lora_node_id
                    current_clip_node = lora_node_id
                    
                    # å¦‚æœæœ‰è§¦å‘è¯ï¼Œæ·»åŠ åˆ°æè¿°ä¸­
                    if trigger_word and trigger_word not in description:
                        description = f"{trigger_word}, {description}"
                        print(f"ğŸ”¤ æ·»åŠ è§¦å‘è¯: {trigger_word}")
                
                # æ›´æ–°KSamplerå’ŒCLIPTextEncodeçš„è¿æ¥ - ä¼˜åŒ–ç«¯å£è¿æ¥
                workflow["31"]["inputs"]["model"] = [current_model_node, 0]
                workflow["6"]["inputs"]["clip"] = [current_clip_node, 1]  # LoraLoaderçš„CLIPè¾“å‡ºæ˜¯ç«¯å£1
                workflow["6"]["inputs"]["text"] = description
                
                print(f"âœ… LoRAèŠ‚ç‚¹è¿æ¥å®Œæˆ: UNET -> {current_model_node}, CLIP -> {current_clip_node}")
        else:
            print("â„¹ï¸ æœªæ£€æµ‹åˆ°LoRAé…ç½®ï¼Œä½¿ç”¨é»˜è®¤å·¥ä½œæµ")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‚è€ƒå›¾
        has_reference_image = reference_image_path and reference_image_path.strip() and not reference_image_path.endswith('blank.png') and reference_image_path != ""
        
        if has_reference_image:
            print("æ£€æµ‹åˆ°å‚è€ƒå›¾ï¼Œä½¿ç”¨å‚è€ƒå›¾æ¨¡å¼")
            # æ›´æ–°å‚è€ƒå›¾åƒè·¯å¾„ - å°†ä¸Šä¼ çš„å›¾åƒå¤åˆ¶åˆ°ComfyUIè¾“å‡ºç›®å½•å¹¶ä½¿ç”¨[output]åç¼€
            container_path = Path(reference_image_path)
            # ç»Ÿä¸€è·¯å¾„åˆ†éš”ç¬¦ï¼Œç¡®ä¿èƒ½æ­£ç¡®åŒ¹é…
            normalized_path = str(container_path).replace('\\', '/')
            if normalized_path.startswith('uploads/'):
                # å°†ä¸Šä¼ çš„å›¾åƒå‹ç¼©åˆ°512x512å¹¶å¤åˆ¶åˆ°ComfyUIè¾“å‡ºç›®å½•
                import shutil
                from PIL import Image
                import io
                
                source_file = Path(reference_image_path)
                dest_file = COMFYUI_MAIN_OUTPUT_DIR / source_file.name
                
                try:
                    # ä½¿ç”¨PILå‹ç¼©å›¾åƒåˆ°512x512
                    with Image.open(source_file) as img:
                        # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if img.mode != 'RGB':
                            img = img.convert('RGB')
                        
                        # å‹ç¼©åˆ°512x512ï¼Œä¿æŒå®½é«˜æ¯”
                        img.thumbnail((TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT), Image.Resampling.LANCZOS)
                        
                        # åˆ›å»º512x512çš„ç™½è‰²èƒŒæ™¯
                        background = Image.new('RGB', (TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT), (255, 255, 255))
                        
                        # å°†å‹ç¼©åçš„å›¾åƒå±…ä¸­æ”¾ç½®
                        offset = ((TARGET_IMAGE_WIDTH - img.width) // 2, (TARGET_IMAGE_HEIGHT - img.height) // 2)
                        background.paste(img, offset)
                        
                        # ä¿å­˜å‹ç¼©åçš„å›¾åƒ
                        background.save(dest_file, 'PNG')
                    
                    print(f"âœ… å‚è€ƒå›¾å‹ç¼©åˆ°512x512å¹¶ä¿å­˜æˆåŠŸ: {source_file} -> {dest_file}")
                except Exception as e:
                    print(f"âŒ å‚è€ƒå›¾å‹ç¼©å¤±è´¥: {e}")
                    print(f"ğŸ“ æºæ–‡ä»¶: {source_file}")
                    print(f"ğŸ“ ç›®æ ‡æ–‡ä»¶: {dest_file}")
                    raise Exception(f"æ— æ³•å‹ç¼©å‚è€ƒå›¾åƒåˆ°{TARGET_IMAGE_WIDTH}x{TARGET_IMAGE_HEIGHT}: {e}")
                
                # ä½¿ç”¨æ–‡ä»¶ååŠ ä¸Š[output]åç¼€
                image_filename = f"{source_file.name} [output]"
                print(f"è®¾ç½®LoadImageOutputå›¾åƒè·¯å¾„: {image_filename}")
                
                # æ·»åŠ LoadImageOutputèŠ‚ç‚¹
                workflow["142"] = {
                    "inputs": {
                        "image": image_filename,
                        "refresh": "refresh"
                    },
                    "class_type": "LoadImageOutput",
                    "_meta": {"title": "åŠ è½½å›¾åƒï¼ˆæ¥è‡ªè¾“å‡ºï¼‰"}
                }
                
                # ä½¿ç”¨ImageScaleèŠ‚ç‚¹æ›¿ä»£FluxKontextImageScaleï¼Œå¼ºåˆ¶å›ºå®šå°ºå¯¸
                workflow["42"] = {
                    "inputs": {
                        "image": ["142", 0],
                        "width": TARGET_IMAGE_WIDTH,
                        "height": TARGET_IMAGE_HEIGHT,
                        "crop": "disabled",
                        "upscale_method": "lanczos",
                        "downscale_method": "area"
                    },
                    "class_type": "ImageScale",
                    "_meta": {"title": "å›¾åƒç¼©æ”¾"}
                }
                
                # æ›´æ–°VAEEncodeèŠ‚ç‚¹ä½¿ç”¨FluxKontextImageScaleçš„è¾“å‡º
                workflow["124"]["inputs"]["pixels"] = ["42", 0]
                
                print(f"âœ… é…ç½®å‚è€ƒå›¾æ¨¡å¼å·¥ä½œæµ")
            else:
                print(f"ä½¿ç”¨åŸå§‹è·¯å¾„: {reference_image_path}")
        else:
            print("æœªæ£€æµ‹åˆ°å‚è€ƒå›¾ï¼Œä½¿ç”¨æ— å‚è€ƒå›¾æ¨¡å¼")
            print(f"âœ… é…ç½®æ— å‚è€ƒå›¾æ¨¡å¼å·¥ä½œæµ")
        
        # æ›´æ–°ç”Ÿæˆå‚æ•°
        if parameters.get("steps"):
            workflow["31"]["inputs"]["steps"] = parameters["steps"]
        
        # æ›´æ–°CFGå‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        if parameters.get("cfg"):
            workflow["31"]["inputs"]["cfg"] = parameters["cfg"]
        
        # æ›´æ–°Guidanceå‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
        if parameters.get("guidance"):
            workflow["35"]["inputs"]["guidance"] = parameters["guidance"]
        
        # å¤„ç†å›¾åƒå°ºå¯¸ - æ°¸è¿œä½¿ç”¨512x512
        target_width = TARGET_IMAGE_WIDTH
        target_height = TARGET_IMAGE_HEIGHT
        
        # æ— è®ºæ˜¯å¦æœ‰å‚è€ƒå›¾ï¼Œéƒ½ä½¿ç”¨å›ºå®šçš„512x512å°ºå¯¸
        if "42" in workflow and "inputs" in workflow["42"]:
            workflow["42"]["inputs"]["width"] = target_width
            workflow["42"]["inputs"]["height"] = target_height
            print(f"è®¾ç½®ç”Ÿæˆå›¾ç‰‡å°ºå¯¸ä¸º: {target_width}x{target_height} (å›ºå®šå°ºå¯¸)")
        
        # å¤„ç†ç”Ÿæˆæ•°é‡
        count = parameters.get("count", 1)
        print(f"ç”Ÿæˆæ•°é‡: {count}")
        # è®¾ç½®KSamplerçš„batch_sizeå‚æ•°
        if count > 1:
            workflow["31"]["inputs"]["batch_size"] = count
            print(f"è®¾ç½®batch_sizeä¸º: {count}")
            # ç¡®ä¿SaveImageèŠ‚ç‚¹çš„save_allå‚æ•°ä¸ºtrue
            if "136" in workflow and "inputs" in workflow["136"]:
                workflow["136"]["inputs"]["save_all"] = True
                print(f"è®¾ç½®SaveImageèŠ‚ç‚¹çš„save_allå‚æ•°ä¸ºtrueï¼Œç¡®ä¿ä¿å­˜æ‰€æœ‰æ‰¹æ¬¡å›¾ç‰‡")
        else:
            # ç¡®ä¿å•å¼ å›¾ç‰‡æ—¶batch_sizeä¸º1
            workflow["31"]["inputs"]["batch_size"] = 1
        
        # è®¾ç½®SaveImageèŠ‚ç‚¹çš„æ–‡ä»¶åå‰ç¼€ä¸ºyeepayï¼Œç”¨äºåŒºåˆ†é¡¹ç›®
        if "136" in workflow and "inputs" in workflow["136"]:
            workflow["136"]["inputs"]["filename_prefix"] = "yeepay/yeepay"
            print(f"è®¾ç½®SaveImageæ–‡ä»¶åå‰ç¼€ä¸º: yeepay/yeepay")
        
        # å¤„ç†ç§å­å‚æ•°
        if parameters.get("seed"):
            workflow["31"]["inputs"]["seed"] = parameters["seed"]
            print(f"ä½¿ç”¨æŒ‡å®šç§å­: {parameters['seed']}")
        else:
            # ç”Ÿæˆéšæœºç§å­
            seed = random.randint(1, 2**32 - 1)
            workflow["31"]["inputs"]["seed"] = seed
            print(f"ä½¿ç”¨éšæœºç§å­: {seed}")
        
        print(f"å·¥ä½œæµå‚æ•°æ›´æ–°å®Œæˆ: æè¿°='{description[:50]}...', æ­¥æ•°={workflow['31']['inputs']['steps']}, CFG={workflow['31']['inputs']['cfg']}, å¼•å¯¼={workflow['35']['inputs']['guidance']}")
        
        return workflow
    
    def _create_qwen_workflow(self, reference_image_path: str, description: str, parameters: Dict[str, Any], model_config):
        """åˆ›å»ºQwenå·¥ä½œæµ"""
        print(f"ğŸ¨ åˆ›å»ºQwenå·¥ä½œæµ: {model_config.display_name}")
        
        # åŠ è½½Qwenå·¥ä½œæµæ¨¡æ¿
        if self.template_path and self.template:
            workflow = self.template.copy()
        else:
            # ä½¿ç”¨å†…ç½®çš„Qwenå·¥ä½œæµæ¨¡æ¿
            workflow = self._get_qwen_template()
        
        # æ›´æ–°æ¨¡å‹æ–‡ä»¶è·¯å¾„
        workflow["23"]["widgets_values"][0] = model_config.unet_file  # UNETLoader
        workflow["24"]["widgets_values"][0] = model_config.clip_file  # CLIPLoader
        workflow["22"]["widgets_values"][0] = model_config.vae_file   # VAELoader
        
        # æ›´æ–°æè¿°æ–‡æœ¬
        workflow["25"]["widgets_values"][0] = description
        
        # æ›´æ–°ç”Ÿæˆå‚æ•°
        if parameters.get("steps"):
            workflow["20"]["widgets_values"][2] = parameters["steps"]
        
        if parameters.get("seed"):
            workflow["20"]["widgets_values"][0] = parameters["seed"]
        
        # å¤„ç†LoRAé…ç½®
        loras = parameters.get("loras", [])
        if loras and len(loras) > 0:
            print(f"ğŸ¨ æ£€æµ‹åˆ° {len(loras)} ä¸ªLoRAé…ç½®")
            loras = loras[:4]  # é™åˆ¶æœ€å¤š4ä¸ªLoRA
            
            # æ„å»ºLoRAé…ç½®æ•°ç»„
            lora_config = []
            for lora in loras:
                if lora.get("enabled", True):
                    lora_config.extend([
                        lora.get("name", ""),
                        lora.get("strength_model", 1.0)
                    ])
            
            workflow["33"]["widgets_values"] = lora_config
            print(f"âœ… LoRAé…ç½®å®Œæˆ: {len(lora_config)//2} ä¸ªLoRA")
        
        # æ›´æ–°ä¿å­˜è·¯å¾„
        workflow["28"]["widgets_values"][0] = "yeepay/yeepay"
        
        print(f"âœ… Qwenå·¥ä½œæµåˆ›å»ºå®Œæˆ")
        return workflow
    
    def _get_qwen_template(self):
        """è·å–Qwenå·¥ä½œæµæ¨¡æ¿"""
        # è¿™é‡Œè¿”å›å†…ç½®çš„Qwenå·¥ä½œæµæ¨¡æ¿
        # å®é™…ä½¿ç”¨æ—¶å¯ä»¥ä»æ–‡ä»¶åŠ è½½
        return {
            # ç®€åŒ–çš„Qwenå·¥ä½œæµæ¨¡æ¿
            "20": {
                "type": "KSampler",
                "widgets_values": [287237245922212, "randomize", 20, 3, "euler", "normal", 1]
            },
            "22": {
                "type": "VAELoader", 
                "widgets_values": ["qwen_image_vae.safetensors"]
            },
            "23": {
                "type": "UNETLoader",
                "widgets_values": ["Qwen-Image_1.0", "default"]
            },
            "24": {
                "type": "CLIPLoader",
                "widgets_values": ["qwen_2.5_vl_7b_fp8_scaled.safetensors", "qwen_image", "default"]
            },
            "25": {
                "type": "CLIPTextEncode",
                "widgets_values": ["{{description}}"]
            },
            "28": {
                "type": "SaveImage",
                "widgets_values": ["yeepay/yeepay"]
            },
            "33": {
                "type": "Lora Loader Stack (rgthree)",
                "widgets_values": []
            }
        }
