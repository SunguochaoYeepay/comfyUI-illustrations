#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Fluxå·¥ä½œæµå®ç°
ä¸“é—¨å¤„ç†Flux Kontextæ¨¡å‹çš„å·¥ä½œæµåˆ›å»º
"""

import random
from typing import Any, Dict

from config.settings import (
    TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT, 
    DEFAULT_STEPS, DEFAULT_COUNT
)

from .base_workflow import BaseWorkflow


class FluxWorkflow(BaseWorkflow):
    """Fluxå·¥ä½œæµåˆ›å»ºå™¨"""
    
    def create_workflow(self, reference_image_path: str, description: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºFluxå·¥ä½œæµ
        
        Args:
            reference_image_path: å‚è€ƒå›¾åƒè·¯å¾„
            description: å›¾åƒæè¿°
            parameters: ç”Ÿæˆå‚æ•°
            
        Returns:
            Fluxå·¥ä½œæµå­—å…¸
        """
        print(f"ğŸ¨ åˆ›å»ºFluxå·¥ä½œæµ: {self.model_config.display_name}")
        
        # éªŒè¯å‚æ•°
        validated_params = self._validate_parameters(parameters)
        
        # å¤„ç†å‚è€ƒå›¾åƒ
        processed_image_path = self._process_reference_image(reference_image_path)
        
        # åˆ›å»ºåŸºç¡€å·¥ä½œæµ
        workflow = self._create_base_workflow(description, validated_params)
        
        # å¤„ç†LoRAé…ç½®
        loras = validated_params.get("loras", [])
        if loras:
            workflow = self._add_lora_nodes(workflow, loras, description)
        
        # å¤„ç†å‚è€ƒå›¾åƒ
        if processed_image_path:
            workflow = self._add_reference_image_nodes(workflow, processed_image_path)
        
        # æ›´æ–°æœ€ç»ˆå‚æ•°
        workflow = self._update_final_parameters(workflow, validated_params)
        
        print(f"âœ… Fluxå·¥ä½œæµåˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(workflow)} ä¸ªèŠ‚ç‚¹")
        return workflow
    
    def _create_base_workflow(self, description: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºåŸºç¡€Fluxå·¥ä½œæµ"""
        workflow = {
            "6": {
                "inputs": {
                    "text": description,
                    "clip": ["38", 0]
                },
                "class_type": "CLIPTextEncode",
                "_meta": {"title": "CLIPæ–‡æœ¬ç¼–ç å™¨"}
            },
            "8": {
                "inputs": {
                    "samples": ["31", 0],
                    "vae": ["39", 0]
                },
                "class_type": "VAEDecode",
                "_meta": {"title": "VAEè§£ç "}
            },
            "31": {
                "inputs": {
                    "seed": parameters.get("seed", random.randint(1, 2**32 - 1)),
                    "steps": parameters.get("steps", DEFAULT_STEPS),
                    "cfg": 1,
                    "sampler_name": "euler",
                    "scheduler": "simple",
                    "denoise": 1,
                    "batch_size": parameters.get("count", DEFAULT_COUNT),
                    "model": ["37", 0],
                    "positive": ["35", 0],
                    "negative": ["135", 0],
                    "latent_image": ["124", 0]
                },
                "class_type": "KSampler",
                "_meta": {"title": "Ké‡‡æ ·å™¨"}
            },
            "35": {
                "inputs": {
                    "guidance": 2.5,
                    "conditioning": ["177", 0]
                },
                "class_type": "FluxGuidance",
                "_meta": {"title": "Fluxå¼•å¯¼"}
            },
            "37": {
                "inputs": {
                    "unet_name": self.model_config.unet_file,
                    "weight_dtype": "default"
                },
                "class_type": "UNETLoader",
                "_meta": {"title": "UNETåŠ è½½å™¨"}
            },
            "38": {
                "inputs": {
                    "clip_name1": "clip_l.safetensors",
                    "clip_name2": "t5xxl_fp8_e4m3fn_scaled.safetensors",
                    "type": "flux",
                    "device": "default"
                },
                "class_type": "DualCLIPLoader",
                "_meta": {"title": "åŒCLIPåŠ è½½å™¨"}
            },
            "39": {
                "inputs": {
                    "vae_name": "ae.safetensors"
                },
                "class_type": "VAELoader",
                "_meta": {"title": "VAEåŠ è½½å™¨"}
            },
            "42": {
                "inputs": {
                    "width": TARGET_IMAGE_WIDTH,
                    "height": TARGET_IMAGE_HEIGHT,
                    "batch_size": 1,
                    "color": 0
                },
                "class_type": "EmptyImage",
                "_meta": {"title": "ç©ºå›¾åƒ"}
            },
            "124": {
                "inputs": {
                    "pixels": ["42", 0],
                    "vae": ["39", 0]
                },
                "class_type": "VAEEncode",
                "_meta": {"title": "VAEç¼–ç "}
            },
            "135": {
                "inputs": {
                    "conditioning": ["6", 0]
                },
                "class_type": "ConditioningZeroOut",
                "_meta": {"title": "æ¡ä»¶é›¶åŒ–"}
            },
            "136": {
                "inputs": {
                    "filename_prefix": "yeepay/yeepay",
                    "images": ["8", 0],
                    "save_all": True
                },
                "class_type": "SaveImage",
                "_meta": {"title": "ä¿å­˜å›¾åƒ"}
            },
            "177": {
                "inputs": {
                    "conditioning": ["6", 0],
                    "latent": ["124", 0]
                },
                "class_type": "ReferenceLatent",
                "_meta": {"title": "ReferenceLatent"}
            }
        }
        
        return workflow
    
    def _add_lora_nodes(self, workflow: Dict[str, Any], loras: list, description: str) -> Dict[str, Any]:
        """æ·»åŠ LoRAèŠ‚ç‚¹"""
        processed_loras = self._process_loras(loras)
        
        if not processed_loras:
            return workflow
        
        print(f"ğŸ¨ æ£€æµ‹åˆ° {len(processed_loras)} ä¸ªLoRAé…ç½®")
        
        current_model_node = "37"  # UNETLoader
        current_clip_node = "38"   # DualCLIPLoader
        
        for i, lora_config in enumerate(processed_loras):
            lora_node_id = str(50 + i)  # 50, 51, 52, 53
            lora_name = lora_config["name"]
            strength_model = lora_config["strength_model"]
            strength_clip = lora_config["strength_clip"]
            trigger_word = lora_config["trigger_word"]
            
            print(f"ğŸ¨ æ·»åŠ LoRA {i+1}: {lora_name} (UNET: {strength_model}, CLIP: {strength_clip})")
            
            # æ·»åŠ LoRAèŠ‚ç‚¹
            workflow[lora_node_id] = {
                "inputs": {
                    "model": [current_model_node, 0],
                    "clip": [current_clip_node, 0],
                    "lora_name": lora_name,
                    "strength_model": strength_model,
                    "strength_clip": strength_clip
                },
                "class_type": "LoraLoader",
                "_meta": {"title": f"LoRAåŠ è½½å™¨{i+1}"}
            }
            
            # æ›´æ–°å½“å‰èŠ‚ç‚¹å¼•ç”¨
            current_model_node = lora_node_id
            current_clip_node = lora_node_id
            
            # æ·»åŠ è§¦å‘è¯
            if trigger_word and trigger_word not in description:
                description = f"{trigger_word}, {description}"
                print(f"ğŸ”¤ æ·»åŠ è§¦å‘è¯: {trigger_word}")
        
        # æ›´æ–°è¿æ¥
        workflow["31"]["inputs"]["model"] = [current_model_node, 0]
        workflow["6"]["inputs"]["clip"] = [current_clip_node, 1]
        workflow["6"]["inputs"]["text"] = description
        
        print(f"âœ… LoRAèŠ‚ç‚¹è¿æ¥å®Œæˆ: UNET -> {current_model_node}, CLIP -> {current_clip_node}")
        return workflow
    
    def _add_reference_image_nodes(self, workflow: Dict[str, Any], image_path: str) -> Dict[str, Any]:
        """æ·»åŠ å‚è€ƒå›¾åƒèŠ‚ç‚¹"""
        print("æ£€æµ‹åˆ°å‚è€ƒå›¾ï¼Œä½¿ç”¨å‚è€ƒå›¾æ¨¡å¼")
        
        # æ·»åŠ LoadImageOutputèŠ‚ç‚¹
        workflow["142"] = {
            "inputs": {
                "image": image_path,
                "refresh": "refresh"
            },
            "class_type": "LoadImageOutput",
            "_meta": {"title": "åŠ è½½å›¾åƒï¼ˆæ¥è‡ªè¾“å‡ºï¼‰"}
        }
        
        # æ›´æ–°ImageScaleèŠ‚ç‚¹
        workflow["42"] = {
            "inputs": {
                "image": ["142", 0],
                "width": TARGET_IMAGE_WIDTH,
                "height": TARGET_IMAGE_HEIGHT,
                "crop": "disabled",
                "upscale_method": "lanczos",
                "downscale_method": "area"
            },
            "class_type": "ImageScale",
            "_meta": {"title": "å›¾åƒç¼©æ”¾"}
        }
        
        # æ›´æ–°VAEEncodeèŠ‚ç‚¹
        workflow["124"]["inputs"]["pixels"] = ["42", 0]
        
        print(f"âœ… é…ç½®å‚è€ƒå›¾æ¨¡å¼å·¥ä½œæµ")
        return workflow
    
    def _update_final_parameters(self, workflow: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
        """æ›´æ–°æœ€ç»ˆå‚æ•°"""
        # æ›´æ–°ç”Ÿæˆå‚æ•°
        if parameters.get("steps"):
            workflow["31"]["inputs"]["steps"] = parameters["steps"]
        
        if parameters.get("cfg"):
            workflow["31"]["inputs"]["cfg"] = parameters["cfg"]
        
        if parameters.get("guidance"):
            workflow["35"]["inputs"]["guidance"] = parameters["guidance"]
        
        # å¤„ç†ç”Ÿæˆæ•°é‡
        count = parameters.get("count", 1)
        workflow["31"]["inputs"]["batch_size"] = count
        
        if count > 1:
            workflow["136"]["inputs"]["save_all"] = True
            print(f"è®¾ç½®batch_sizeä¸º: {count}")
        
        # è®¾ç½®ç§å­
        if parameters.get("seed"):
            workflow["31"]["inputs"]["seed"] = parameters["seed"]
            print(f"ä½¿ç”¨æŒ‡å®šç§å­: {parameters['seed']}")
        else:
            seed = random.randint(1, 2**32 - 1)
            workflow["31"]["inputs"]["seed"] = seed
            print(f"ä½¿ç”¨éšæœºç§å­: {seed}")
        
        print(f"å·¥ä½œæµå‚æ•°æ›´æ–°å®Œæˆ: æ­¥æ•°={workflow['31']['inputs']['steps']}, CFG={workflow['31']['inputs']['cfg']}, å¼•å¯¼={workflow['35']['inputs']['guidance']}")
        return workflow
