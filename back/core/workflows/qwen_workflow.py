#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwenå·¥ä½œæµå®ç°
ä¸“é—¨å¤„ç†Qwenæ¨¡å‹çš„å·¥ä½œæµåˆ›å»º
"""

import json
from typing import Any, Dict

from .base_workflow import BaseWorkflow


class QwenWorkflow(BaseWorkflow):
    """Qwenå·¥ä½œæµåˆ›å»ºå™¨"""
    
    def create_workflow(self, reference_image_path: str, description: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºQwenå·¥ä½œæµ
        
        Args:
            reference_image_path: å‚è€ƒå›¾åƒè·¯å¾„
            description: å›¾åƒæè¿°
            parameters: ç”Ÿæˆå‚æ•°
            
        Returns:
            Qwenå·¥ä½œæµå­—å…¸
        """
        print(f"ğŸ¨ åˆ›å»ºQwenå·¥ä½œæµ: {self.model_config.display_name}")
        
        # éªŒè¯å‚æ•°
        validated_params = self._validate_parameters(parameters)
        
        # åŠ è½½å·¥ä½œæµæ¨¡æ¿
        workflow = self._load_workflow_template()
        
        # æ›´æ–°æ¨¡å‹é…ç½®
        workflow = self._update_model_config(workflow)
        
        # æ›´æ–°æ–‡æœ¬æè¿°
        workflow = self._update_text_description(workflow, description)
        
        # æ›´æ–°é‡‡æ ·å‚æ•°
        workflow = self._update_sampling_parameters(workflow, validated_params)
        
        # æ›´æ–°ä¿å­˜è·¯å¾„
        workflow = self._update_save_path(workflow)
        
        # å¤„ç†LoRAé…ç½®
        loras = validated_params.get("loras", [])
        if loras:
            workflow = self._update_lora_config(workflow, loras)
        
        print(f"âœ… Qwenå·¥ä½œæµåˆ›å»ºå®Œæˆï¼Œä½¿ç”¨æ ‡å‡†ComfyUIæ ¼å¼")
        return workflow
    
    def _load_workflow_template(self) -> Dict[str, Any]:
        """åŠ è½½å·¥ä½œæµæ¨¡æ¿"""
        try:
            workflow_path = "workflows/qwen_image_generation_workflow.json"
            with open(workflow_path, 'r', encoding='utf-8') as f:
                workflow = json.load(f)
            print(f"âœ… åŠ è½½Qwenå·¥ä½œæµæ¨¡æ¿: {workflow_path}")
            return workflow
        except FileNotFoundError:
            print(f"âš ï¸ Qwenå·¥ä½œæµæ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å†…ç½®æ¨¡æ¿")
            return self._get_builtin_template()
        except json.JSONDecodeError as e:
            print(f"âŒ Qwenå·¥ä½œæµæ¨¡æ¿æ–‡ä»¶æ ¼å¼é”™è¯¯: {str(e)}")
            return self._get_builtin_template()
    
    def _get_builtin_template(self) -> Dict[str, Any]:
        """è·å–å†…ç½®æ¨¡æ¿"""
        from config.settings import TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT
        
        return {
            "20": {
                "type": "KSampler",
                "inputs": {
                    "seed": 287237245922212,
                    "steps": 20,
                    "cfg": 3,
                    "sampler_name": "euler",
                    "scheduler": "normal",
                    "denoise": 1
                }
            },
            "22": {
                "type": "VAELoader", 
                "inputs": {
                    "vae_name": "qwen_image_vae.safetensors"
                }
            },
            "23": {
                "type": "UNETLoader",
                "inputs": {
                    "unet_name": "Qwen-Image_1.0",
                    "weight_dtype": "default"
                }
            },
            "24": {
                "type": "CLIPLoader",
                "inputs": {
                    "clip_name": "qwen_2.5_vl_7b_fp8_scaled.safetensors",
                    "clip_type": "qwen_image",
                    "weight_dtype": "default"
                }
            },
            "25": {
                "type": "CLIPTextEncode",
                "inputs": {
                    "text": "{{description}}"
                }
            },
            "27": {
                "type": "CR SDXL Aspect Ratio",
                "inputs": {
                    "width": TARGET_IMAGE_WIDTH,
                    "height": TARGET_IMAGE_HEIGHT,
                    "aspect_ratio": "custom",
                    "swap_dimensions": "Off",
                    "upscale_factor": 1,
                    "batch_size": 1
                }
            },
            "28": {
                "type": "SaveImage",
                "inputs": {
                    "filename_prefix": "yeepay/yeepay"
                }
            },
            "33": {
                "type": "Lora Loader Stack (rgthree)",
                "inputs": {
                    "lora_01": "None",
                    "strength_01": 0.8,
                    "lora_02": "None",
                    "strength_02": 0.1,
                    "lora_03": "None",
                    "strength_03": 0.1,
                    "lora_04": "None",
                    "strength_04": 0.1
                }
            }
        }
    
    def _update_model_config(self, workflow: Dict[str, Any]) -> Dict[str, Any]:
        """æ›´æ–°æ¨¡å‹é…ç½®"""
        if "23" in workflow:
            workflow["23"]["inputs"]["unet_name"] = self.model_config.unet_file
            print(f"âœ… æ›´æ–°UNETLoader: {self.model_config.unet_file}")
        
        if "24" in workflow:
            workflow["24"]["inputs"]["clip_name"] = self.model_config.clip_file
            print(f"âœ… æ›´æ–°CLIPLoader: {self.model_config.clip_file}")
        
        if "22" in workflow:
            workflow["22"]["inputs"]["vae_name"] = self.model_config.vae_file
            print(f"âœ… æ›´æ–°VAELoader: {self.model_config.vae_file}")
        
        return workflow
    
    def _update_text_description(self, workflow: Dict[str, Any], description: str) -> Dict[str, Any]:
        """æ›´æ–°æ–‡æœ¬æè¿°"""
        if "25" in workflow:
            workflow["25"]["inputs"]["text"] = description
            print(f"âœ… æ›´æ–°æè¿°æ–‡æœ¬: {description[:50]}...")
        
        return workflow
    
    def _update_sampling_parameters(self, workflow: Dict[str, Any], parameters: Dict[str, Any]) -> Dict[str, Any]:
        """æ›´æ–°é‡‡æ ·å‚æ•°"""
        if "20" in workflow:
            if parameters.get("steps"):
                workflow["20"]["inputs"]["steps"] = parameters["steps"]
            if parameters.get("seed"):
                workflow["20"]["inputs"]["seed"] = parameters["seed"]
            print(f"âœ… æ›´æ–°KSamplerå‚æ•°: æ­¥æ•°={parameters.get('steps', 20)}, ç§å­={parameters.get('seed', 'random')}")
        
        # åŠ¨æ€æ›´æ–°å›¾åƒå°ºå¯¸é…ç½®
        workflow = self._update_image_dimensions(workflow)
        
        return workflow
    
    def _update_image_dimensions(self, workflow: Dict[str, Any]) -> Dict[str, Any]:
        """åŠ¨æ€æ›´æ–°å›¾åƒå°ºå¯¸é…ç½®"""
        from config.settings import TARGET_IMAGE_WIDTH, TARGET_IMAGE_HEIGHT
        
        # æ›´æ–°èŠ‚ç‚¹27ï¼ˆCR SDXL Aspect Ratioï¼‰çš„å°ºå¯¸é…ç½®
        if "27" in workflow:
            workflow["27"]["inputs"]["width"] = TARGET_IMAGE_WIDTH
            workflow["27"]["inputs"]["height"] = TARGET_IMAGE_HEIGHT
            # ä½¿ç”¨customé€‰é¡¹ï¼Œé€šè¿‡widthå’Œheightå‚æ•°æ§åˆ¶å°ºå¯¸
            workflow["27"]["inputs"]["aspect_ratio"] = "custom"
            print(f"âœ… åŠ¨æ€æ›´æ–°å›¾åƒå°ºå¯¸: {TARGET_IMAGE_WIDTH}x{TARGET_IMAGE_HEIGHT} (è‡ªå®šä¹‰)")
        
        return workflow
    
    def _update_save_path(self, workflow: Dict[str, Any]) -> Dict[str, Any]:
        """æ›´æ–°ä¿å­˜è·¯å¾„"""
        if "28" in workflow:
            workflow["28"]["inputs"]["filename_prefix"] = "yeepay/yeepay"
            print(f"âœ… æ›´æ–°ä¿å­˜è·¯å¾„: yeepay/yeepay")
        
        return workflow
    
    def _update_lora_config(self, workflow: Dict[str, Any], loras: list) -> Dict[str, Any]:
        """æ›´æ–°LoRAé…ç½®"""
        if "33" not in workflow:
            return workflow
        
        processed_loras = self._process_loras(loras)
        
        if not processed_loras:
            print("â„¹ï¸ æœªæ£€æµ‹åˆ°LoRAé…ç½®ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
            return workflow
        
        print(f"ğŸ¨ æ£€æµ‹åˆ° {len(processed_loras)} ä¸ªLoRAé…ç½®")
        
        # é‡ç½®æ‰€æœ‰LoRAé…ç½®
        workflow["33"]["inputs"]["lora_01"] = "None"
        workflow["33"]["inputs"]["strength_01"] = 0.8
        workflow["33"]["inputs"]["lora_02"] = "None"
        workflow["33"]["inputs"]["strength_02"] = 0.1
        workflow["33"]["inputs"]["lora_03"] = "None"
        workflow["33"]["inputs"]["strength_03"] = 0.1
        workflow["33"]["inputs"]["lora_04"] = "None"
        workflow["33"]["inputs"]["strength_04"] = 0.1
        
        # è®¾ç½®å¯ç”¨çš„LoRA
        for i, lora in enumerate(processed_loras):
            if i >= 4:  # é™åˆ¶æœ€å¤š4ä¸ªLoRA
                break
                
            lora_key = f"lora_{i+1:02d}"
            strength_key = f"strength_{i+1:02d}"
            
            workflow["33"]["inputs"][lora_key] = lora["name"]
            workflow["33"]["inputs"][strength_key] = lora["strength_model"]
            print(f"âœ… è®¾ç½®LoRA {i+1}: {lora['name']} (å¼ºåº¦: {lora['strength_model']})")
        
        print(f"âœ… LoRAé…ç½®å®Œæˆ: {len(processed_loras)} ä¸ªLoRA")
        return workflow
